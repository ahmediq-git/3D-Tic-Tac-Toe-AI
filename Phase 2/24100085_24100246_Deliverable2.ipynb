{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w6UT0U1YyYP"
      },
      "source": [
        "# Deliverable 2\n",
        "\n",
        "## By:\n",
        "## Ahmed Mozammil Iqbal (Roll no. 24100085)\n",
        "## Javeria Siddique (Roll no. 24100246)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dGaLUR-uYuL_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ujson in c:\\users\\jiyaa\\miniconda3\\envs\\ds1\\lib\\site-packages (5.9.0)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "!pip install ujson\n",
        "import ujson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sE4PDPSFYuMB"
      },
      "outputs": [],
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = [[\" \" for _ in range(4)] for _ in range(4)]\n",
        "        self.current_player = -1\n",
        "\n",
        "\n",
        "    def check_draw(self):\n",
        "        for row in self.board:\n",
        "            if \" \" in row:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def print_board(self):\n",
        "        # Prints a GUI-like representation of the board\n",
        "        print(\"┌───┬───┬───┬───┐\")\n",
        "        for i, row in enumerate(self.board):\n",
        "            print(\"│ \" + \" │ \".join(row) + \" │\")\n",
        "            if i < 3:\n",
        "                print(\"├───┼───┼───┼───┤\")\n",
        "        print(\"└───┴───┴───┴───┘\")\n",
        "\n",
        "    def check_winner(self, player):\n",
        "        for row in self.board:\n",
        "            if all([cell == player for cell in row]):\n",
        "                return True\n",
        "        for col in range(4):\n",
        "            if all([self.board[row][col] == player for row in range(4)]):\n",
        "                return True\n",
        "        if all([self.board[i][i] == player for i in range(4)]) or all(\n",
        "            [self.board[i][3 - i] == player for i in range(4)]\n",
        "        ):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def step(self, state):\n",
        "        row = int(state / 4)\n",
        "        col = int(state % 4)\n",
        "\n",
        "        current_player_symbol = \" \"\n",
        "        if self.current_player == -1:\n",
        "            current_player_symbol = \"X\"\n",
        "        else:\n",
        "            current_player_symbol = \"O\"\n",
        "\n",
        "        if self.board[row][col] == \" \":\n",
        "            self.board[row][col] = current_player_symbol\n",
        "\n",
        "        if self.check_winner(\"O\"):\n",
        "            return self.board, self.current_player, True, -1\n",
        "        elif self.check_winner(\"X\"):\n",
        "            return self.board, self.current_player, True, 1\n",
        "        elif self.check_draw():\n",
        "            return self.board, self.current_player, True, 0\n",
        "\n",
        "        self.current_player *= -1\n",
        "\n",
        "        return self.board, self.current_player, False, 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DRd1CzkPYuMD"
      },
      "outputs": [],
      "source": [
        "# Initialized as a random policy for player 1\n",
        "\n",
        "# def policy_player1(board):\n",
        "\n",
        "#     possible_actions = []\n",
        "\n",
        "#     for i in range(4):\n",
        "#         for j in range(4):\n",
        "#             if board[i][j] == \" \":\n",
        "#                 possible_actions.append(i*4 + j)\n",
        "\n",
        "\n",
        "#     return random.choice(possible_actions)\n",
        "\n",
        "\n",
        "\n",
        "# # Initialized as a random policy for player 2\n",
        "# def policy_player2(board):\n",
        "\n",
        "#     possible_actions = []\n",
        "\n",
        "#     for i in range(4):\n",
        "#         for j in range(4):\n",
        "#             if board[i][j] == \" \":\n",
        "#                 possible_actions.append(i*4 + j)\n",
        "\n",
        "\n",
        "\n",
        "#     return random.choice(possible_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6sPYEjgumLV"
      },
      "source": [
        "# Q LEARNING (training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qi5kqpU2uloN"
      },
      "outputs": [],
      "source": [
        "#Helper functions\n",
        "\n",
        "#Generate all 4 by 4 states\n",
        "\n",
        "# We generate all the states possible for the algorithm. We have used meshgrid as it is more optimized than our previous recursive definition\n",
        "def generate_all_states():\n",
        "    values = [-1, 0, 1]\n",
        "    combinations = np.array(np.meshgrid(*([values] * 16))).T.reshape(-1, 16)\n",
        "    return combinations.tolist()\n",
        "\n",
        "# Removes all impossible states\n",
        "def remove_impossible_states(allStates):\n",
        "    allStates = np.array(allStates)\n",
        "\n",
        "    # Count occurrences of -1 and 1 along axis 1\n",
        "    numX = np.count_nonzero(allStates == -1, axis=1)\n",
        "    numO = np.count_nonzero(allStates == 1, axis=1)\n",
        "\n",
        "    # This makes a mask of the states which should be allowed\n",
        "    mask = np.logical_or(numX == numO, np.logical_or(numO == numX - 1, np.logical_and(numX == 0, numO == 0)))\n",
        "\n",
        "    # We apply the mask to filter out all illegal states\n",
        "    reducedStates = allStates[mask]\n",
        "\n",
        "\n",
        "    # We remove the states where both players are winning\n",
        "    finalStates = [tuple(state) for state in reducedStates if not (player_wins(state, -1) and player_wins(state, 1))]\n",
        "\n",
        "    print(\"all states: \", len(allStates))\n",
        "    print(\"final states: \", len(finalStates))\n",
        "\n",
        "    return finalStates\n",
        "\n",
        "# player is either -1 (X) or 1 (O)\n",
        "# we check if X wins or O wins with this function, will be used to get the final derived states\n",
        "# (We split it in multiple If statement for better readibility)\n",
        "def player_wins(state, player):\n",
        "    # Check rows\n",
        "    if state[0] == player and state[1] == player and state[2] == player and state[3] == player:\n",
        "        return True\n",
        "    elif state[4] == player and state[5] == player and state[6] == player and state[7] == player:\n",
        "        return True\n",
        "    elif state[8] == player and state[9] == player and state[10] == player and state[11] == player:\n",
        "        return True\n",
        "    elif state[12] == player and state[13] == player and state[14] == player and state[15] == player:\n",
        "        return True\n",
        "\n",
        "    # Check columns\n",
        "    elif state[0] == player and state[4] == player and state[8] == player and state[12] == player:\n",
        "        return True\n",
        "    elif state[1] == player and state[5] == player and state[9] == player and state[13] == player:\n",
        "        return True\n",
        "    elif state[2] == player and state[6] == player and state[10] == player and state[14] == player:\n",
        "        return True\n",
        "    elif state[3] == player and state[7] == player and state[11] == player and state[15] == player:\n",
        "        return True\n",
        "\n",
        "    # Check diagonals\n",
        "    elif state[0] == player and state[5] == player and state[10] == player and state[15] == player:\n",
        "        return True\n",
        "    elif state[3] == player and state[6] == player and state[9] == player and state[12] == player:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Given a state this will find the next possible states\n",
        "def next_possible_states(state, playeraction):\n",
        "    zero_indexes = np.where(np.array(state) == 0)[0]\n",
        "    nextstates = [np.copy(state) for _ in zero_indexes]\n",
        "\n",
        "    for i, pos_nextaction in enumerate(zero_indexes):\n",
        "        nextstates[i][pos_nextaction] = playeraction\n",
        "\n",
        "    return nextstates, zero_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k3enoos9xSXU"
      },
      "outputs": [],
      "source": [
        "# all_states=remove_impossible_states(generate_all_states())\n",
        "# print(\"All states (allowed + not allowed) generated\")\n",
        "# all_states = np.array(all_states)\n",
        "# np.save(\"all_states.npy\", all_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "saZ7oEFzvrpD"
      },
      "outputs": [],
      "source": [
        "# # Initialization\n",
        "# #Initialising all the tables\n",
        "\n",
        "# # For computing policy table\n",
        "# policy_table={}\n",
        "# for state in all_states:\n",
        "#     policy_table[state]=None\n",
        "# print(\"policy_table initialized\")\n",
        "\n",
        "# with open('policy_table.txt', 'w') as file:\n",
        "#     file.write(ujson.dumps(policy_table))\n",
        "\n",
        "# # Reward Table\n",
        "# reward_table={}\n",
        "# for state in all_states:\n",
        "#     if player_wins(state, -1):\n",
        "#         reward_table[state]=1\n",
        "#     elif player_wins(state, 1):\n",
        "#         reward_table[state]=-1\n",
        "#     else:\n",
        "#         reward_table[state]=0\n",
        "# print(\"reward_table initialized\")\n",
        "\n",
        "# with open('reward_table.txt', 'w') as file:\n",
        "#     file.write(ujson.dumps(reward_table))\n",
        "\n",
        "# # Turns Table: says whose turn it is at each instance\n",
        "# turns_table = {}\n",
        "# for state in all_states:\n",
        "#     if np.all(np.array(state) == 0):\n",
        "#         turns_table[state] = -1\n",
        "#     elif np.count_nonzero(np.array(state) == -1) > np.count_nonzero(np.array(state) == 1):\n",
        "#         turns_table[state] = 1\n",
        "#     else:\n",
        "#         turns_table[state] = -1\n",
        "# print(\"turns_table initialized\")\n",
        "\n",
        "# with open('turns_table.txt', 'w') as file:\n",
        "#     file.write(ujson.dumps(turns_table))\n",
        "\n",
        "# # Stores the next state of all the states\n",
        "# # convert to dictionary of state: {action:nextstate, action2:....}\n",
        "# next_state_from_action={}\n",
        "# action_locations={} #stores places of next action corresponding to the next_states in question\n",
        "# all_next_states={}\n",
        "# for state in all_states:\n",
        "#     whose_turn=turns_table[str(state)]\n",
        "#     all_next_states[state], action_locations[state] =  next_possible_states(state, whose_turn)\n",
        "\n",
        "#     next_state_from_action[state]={action:state_from_action for action, state_from_action in zip(action_locations[state], all_next_states[state])}\n",
        "# print(\"next_states and action_locations initialised\")\n",
        "\n",
        "\n",
        "# str_action_locations = {}\n",
        "# for key, value in action_locations.items():\n",
        "#     str_action_locations[str(key)] = str(tuple(value))\n",
        "\n",
        "# with open('action_locations.txt', 'w') as file:\n",
        "#     file.write(ujson.dumps(str_action_locations))\n",
        "\n",
        "# str_next_state_from_action = {}\n",
        "# for key in next_state_from_action.keys():\n",
        "#     str_next_state_from_action[str(key)] = {}\n",
        "#     for internal_key in next_state_from_action[key].keys():\n",
        "#         str_next_state_from_action[str(key)][internal_key] = str(tuple(next_state_from_action[key][internal_key]))\n",
        "\n",
        "# with open('next_state_from_action.txt', 'w') as file:\n",
        "#     file.write(ujson.dumps(str_next_state_from_action))\n",
        "\n",
        "# #Q table: action is 0, 1, 2, 3,....,16\n",
        "# Q_table={} #state: {action1: value of this state, action2: value of next state2, action3: value of next state3....} according to next_states table\n",
        "# for state in all_states:\n",
        "#     actions = {action: 0 for action in action_locations[state]}\n",
        "#     Q_table[state]=actions\n",
        "#     pass\n",
        "# print(\"Q table initialised\")\n",
        "\n",
        "# with open('q_table.txt', 'w') as file:\n",
        "#     file.write(ujson.dumps(Q_table))\n",
        "\n",
        "# Value_table={}\n",
        "# for state in all_states:\n",
        "#     Value_table[state]=0\n",
        "# print(\"Value table initialised\")\n",
        "\n",
        "# with open('value_table.txt', 'w') as file:\n",
        "#     file.write(ujson.dumps(Value_table))\n",
        "\n",
        "\n",
        "# # Terminal states for the game:\n",
        "\n",
        "# # All states mapped to true or false if terminal or not\n",
        "# Terminal_states={}\n",
        "# for state in all_states:\n",
        "#     if player_wins(state, -1) or player_wins(state, 1):\n",
        "#         Terminal_states[state]=True\n",
        "#         Q_table[state]={}\n",
        "#         all_next_states[state]=[]\n",
        "#         action_locations[state]=[]\n",
        "#         next_state_from_action[state]={}\n",
        "#     elif Q_table[state]=={}:\n",
        "#         Terminal_states[state]=True\n",
        "#     else:\n",
        "#         Terminal_states[state]=False\n",
        "# print(\"Terminal states initialised\")\n",
        "\n",
        "# with open('terminal_states.txt', 'w') as file:\n",
        "#     file.write(ujson.dumps(Terminal_states))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10110713\n"
          ]
        }
      ],
      "source": [
        "all_states = np.load(\"all_states.npy\", allow_pickle=True) \n",
        "all_states = [tuple(state) for state in all_states]\n",
        "policy_table = ujson.load(open('policy_table.txt', 'r'))\n",
        "print(len(policy_table))\n",
        "reward_table = ujson.load(open('reward_table.txt', 'r'))\n",
        "print(len(reward_table))\n",
        "turns_table = ujson.load(open('turns_table.txt', 'r'))\n",
        "print(len(turns_table))\n",
        "next_state_from_action = ujson.load(open('next_state_from_action.txt', 'r'))\n",
        "print(len(next_state_from_action))\n",
        "Q_table = ujson.load(open('q_table.txt', 'r'))\n",
        "print(len(Q_table))\n",
        "Value_table = ujson.load(open('value_table.txt', 'r'))\n",
        "print(len(Value_table))\n",
        "Terminal_states = ujson.load(open('terminal_states.txt', 'r'))\n",
        "print(len(Terminal_states))\n",
        "action_locations = ujson.load(open('action_locations.txt', 'r'))\n",
        "print(len(action_locations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "C49DpdN8wHX0"
      },
      "outputs": [],
      "source": [
        "#The Q learning function (also needs edits)\n",
        "def Qlearning(gamma, alpha, epsilon, total_actions, reward_table, value_table, Q_table, policy_table, epochs):\n",
        "  count = 0\n",
        "  initial_epsilon = epsilon\n",
        "  min_epsilon = 0.1\n",
        "  min_alpha = 0.01\n",
        "  decay_rate = 0.001\n",
        "\n",
        "  for _ in range(epochs):\n",
        "    state=(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\n",
        "    if count % 10000 == 0:\n",
        "      print(count)\n",
        "    while True:\n",
        "      if np.random.rand() < epsilon:\n",
        "        num_actions = action_locations[str(state)].strip('()').split(',')\n",
        "        num_actions = [item.replace(\" \", \"\") for item in num_actions]\n",
        "        num_actions = [action for action in num_actions if action != '']\n",
        "        action = random.choice(num_actions) #at times it may choose a random policy\n",
        "      else:\n",
        "        action = max(Q_table[str(state)], key=Q_table[str(state)].get) #but otherwise it chooses from the learned policies (this is argmax with dictionaries)\n",
        "      next_state=next_state_from_action[str(state)][str(action)] #this is the next state after taking the action\n",
        "      max_value = 0\n",
        "      min_value = 0\n",
        "      if Terminal_states[str(next_state)] == False:\n",
        "        max_value = max(Q_table[str(next_state)].values())   \n",
        "        min_value = min(Q_table[str(next_state)].values())      \n",
        "      reward = reward_table[str(next_state)]\n",
        "\n",
        "      if turns_table[str(state)]==-1:\n",
        "        Q_table[str(state)][str(action)] += alpha * (reward + gamma * max_value - Q_table[str(state)][str(action)])\n",
        "      else:\n",
        "        Q_table[str(state)][str(action)] += alpha * (reward + gamma * min_value - Q_table[str(state)][str(action)])\n",
        "\n",
        "      state=next_state\n",
        "\n",
        "      # we exit if state is one of the terminal states\n",
        "      if Terminal_states[str(state)]==True:\n",
        "        break\n",
        "    count += 1\n",
        "    epsilon = (initial_epsilon - min_epsilon) * max(0, ((epochs - count) / epochs)) + min_epsilon\n",
        "    if count % 1000 == 0:\n",
        "      alpha = max(min_alpha, alpha * (1 - decay_rate))\n",
        "\n",
        "  return Q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n",
            "120000\n",
            "130000\n",
            "140000\n",
            "150000\n",
            "160000\n",
            "170000\n",
            "180000\n",
            "190000\n",
            "200000\n",
            "210000\n",
            "220000\n",
            "230000\n",
            "240000\n",
            "250000\n",
            "260000\n",
            "270000\n",
            "280000\n",
            "290000\n",
            "300000\n",
            "310000\n",
            "320000\n",
            "330000\n",
            "340000\n",
            "350000\n",
            "360000\n",
            "370000\n",
            "380000\n",
            "390000\n",
            "400000\n",
            "410000\n",
            "420000\n",
            "430000\n",
            "440000\n",
            "450000\n",
            "460000\n",
            "470000\n",
            "480000\n",
            "490000\n",
            "500000\n",
            "510000\n",
            "520000\n",
            "530000\n",
            "540000\n",
            "550000\n",
            "560000\n",
            "570000\n",
            "580000\n",
            "590000\n",
            "600000\n",
            "610000\n",
            "620000\n",
            "630000\n",
            "640000\n",
            "650000\n",
            "660000\n",
            "670000\n",
            "680000\n",
            "690000\n",
            "700000\n",
            "710000\n",
            "720000\n",
            "730000\n",
            "740000\n",
            "750000\n",
            "760000\n",
            "770000\n",
            "780000\n",
            "790000\n",
            "800000\n",
            "810000\n",
            "820000\n",
            "830000\n",
            "840000\n",
            "850000\n",
            "860000\n",
            "870000\n",
            "880000\n",
            "890000\n",
            "900000\n",
            "910000\n",
            "920000\n",
            "930000\n",
            "940000\n",
            "950000\n",
            "960000\n",
            "970000\n",
            "980000\n",
            "990000\n"
          ]
        }
      ],
      "source": [
        "Q_table = Qlearning(0.9, 0.9, 0.9, 16, reward_table, Value_table, Q_table, policy_table, 1000000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "for state in Q_table.keys():\n",
        "    if Q_table[state] == {}:\n",
        "        continue\n",
        "    if turns_table[str(state)] == -1:\n",
        "        policy_table[str(state)] = max(Q_table[state], key=Q_table[state].get)\n",
        "    else:\n",
        "        policy_table[str(state)] = min(Q_table[state], key=Q_table[state].get)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('policy_table.txt', 'w') as file:\n",
        "    file.write(ujson.dumps(policy_table))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "JEivp-S5wNF3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10110713"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Functions to save our policy to a file for easy retrieval and access\n",
        "# # For evaluation\n",
        "\n",
        "def load_policies():\n",
        "    # Load the policy table from the file\n",
        "    with open('policy_table.txt', 'r') as file:\n",
        "        policy_table = ujson.load(file)\n",
        "    return policy_table\n",
        "    \n",
        "policy = load_policies()\n",
        "\n",
        "len(policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # function to retrieve policy from the file\n",
        "def get_policy(state):\n",
        "    state = np.array(state).flatten()\n",
        "    state = state.tolist()\n",
        "    for i in range(len(state)):\n",
        "        if state[i] == \" \":\n",
        "            state[i] = 0\n",
        "        elif state[i] == \"X\":\n",
        "            state[i] = -1\n",
        "        elif state[i] == \"O\":\n",
        "            state[i] = 1\n",
        "    print(\"new state: \", state)\n",
        "    state=str(tuple(state))\n",
        "    return policy[state]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJYw2FMFvgMd"
      },
      "source": [
        "# Application of code starts here (was part of already given boiler code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "yD1oupzmYuME"
      },
      "outputs": [],
      "source": [
        "def play_one_game():\n",
        "    tictactoe = TicTacToe()\n",
        "    terminated = 0\n",
        "    board = [[\" \" for _ in range(4)] for _ in range(4)]\n",
        "    for i in range(8):\n",
        "        for turn in [-1, 1]:\n",
        "            action = 0\n",
        "            if turn == -1:\n",
        "                action = get_policy(board)\n",
        "            else:\n",
        "                action = get_policy(board)\n",
        "            board, player, terminated, reward = tictactoe.step(int(action))\n",
        "            # Uncomment this if you want to see the board\n",
        "            tictactoe.print_board()\n",
        "            if terminated:\n",
        "                break\n",
        "    return -1*reward # This is the player who won"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "play_one_game()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDjCkCgeYuME"
      },
      "outputs": [],
      "source": [
        "# def run_alternating_games(games=10):\n",
        "#     results = []\n",
        "#     for i in range(games):\n",
        "#         for j in range(2):\n",
        "#             if j==0:\n",
        "#                 winner = play_one_game(policy_player1, policy_player2)\n",
        "\n",
        "#                 match winner:\n",
        "#                     case -1:\n",
        "#                         results.append(1)\n",
        "#                     case 1:\n",
        "#                         results.append(2)\n",
        "#                     case 0:\n",
        "#                         results.append(0)\n",
        "\n",
        "#             if j==1:\n",
        "#                 winner = play_one_game(policy_player2, policy_player1)\n",
        "\n",
        "#                 match winner:\n",
        "#                     case -1:\n",
        "#                         results.append(2)\n",
        "#                     case 1:\n",
        "#                         results.append(1)\n",
        "#                     case 0:\n",
        "#                         results.append(0)\n",
        "\n",
        "\n",
        "#     return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKjHZ3sTYuMF"
      },
      "outputs": [],
      "source": [
        "# results = run_alternating_games(1000)\n",
        "# print(\"Draws: \", results.count(0))\n",
        "# print(\"Player 1 Wins:\", results.count(1))\n",
        "# print(\"Player 2 Wins:\", results.count(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEUV7MUKYuMG"
      },
      "source": [
        "I have created two functions that randomly select any action from the available actions from the board. Your team will have to create such a function that outputs the optimal action given a particular board state. This a similar kind of code I will be using on competition day when your function will play against an opponent's functions for perhaps a 1000 games."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42lUujcwYuMI"
      },
      "source": [
        "I will pass your and your opponent's function into the run alternating games function for maybe 1000 games to see who won more games. That person will be the winner of the match. I think it's a reliable method to compare policies. Run them by each for 1000s of games and see what policy wins the most games."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUJ88XG9YuMJ"
      },
      "source": [
        "You have to solve this part using **Q Learning**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
